#!/usr/bin/env python

import csv
import dateutil.parser
import json
import os
import sys
import time
import requests
import logging

from datetime import datetime
from collections import OrderedDict
from pymongo import MongoClient

from .add_to_database import *
from .IP import *
from . import file_process as fp
from .vt_process import * 
from .avclassplusplus import * 
from celery import Celery 

celery = Celery('tasks', broker='redis://localhost', backend='redis://localhost')

TYPES = ['malware-url']
NAME = 'URLhaus'
DISABLED = False

def periodic_link():
    while True:
        client, db = connect_database()
        collection = db['malware_collection']
        cursor = collection.find({})
        for document in cursor:
            if document['url_status_malware'] == True:
                try: 
                    r = requests.get(document['url_malware'], stream=True, allow_redirects=True, timeout=10)
                    print("No update of URL status: " + document['url_malware'])
                except:
                    db.malware_collection.update( 
                    { "id_malware" : document['id_malware']},
                        {"$set" : { "url_status": False }
                    })
                    print("Update of URL status: " + document['url_malware'])
            elif document['url_status_malware'] == False:
                try:
                    r = requests.get(document['url_malware'], stream=True, allow_redirects=True, timeout=10)
                    db.malware_collection.update( 
                    { "id_malware" : document['id_malware'] },
                        {"$set" : { "url_status": True }
                    })
                    print("Update of URL status: " + document['url_malware'])
                except:
                    print("No update of URL status: " + document['url_malware'])
            time.sleep(60)

def get_name_file_from_url(url):
    """
    Get name file from url
    Params:
    - url: (type: string).
    Returns:
    - name: (type: string).
    """

    print("\nSearching for filename from url...")
    last_slash = url.rfind('/')
    return url[last_slash+1:]
    print("\nDone.")

def break_csv(lines):
    for line in lines:
        if line.startswith('#'):
            lines.remove(line)
    reader = csv.reader(lines, quotechar='"',delimiter=',',quoting=csv.QUOTE_ALL, skipinitialspace=True)
    next(reader)
    return reader 

def save_infos_from_csv(item):
    '''
    SAVE INFOS FROM CSV 
    Params:
    - item: (type: string) URL.
    Returns:
    - id_malware, date_added, url_malware, url_status, threat, tags, urlhaus_link, reporter.
    '''
    id_malware = item[0]
    date_added = datetime.strptime(item[1], '%Y-%m-%d %H:%M:%S')
    url_malware = clean_url(item[2])
    if item[3] == "online":
        url_status = True
    else:
        url_status = False
    threat = item[4]
    tags = item[5]
    urlhaus_link = item[6]
    reporter = item[7]
    return id_malware, date_added, url_malware, url_status, threat, tags, urlhaus_link, reporter

@celery.task
def add_malware_to_crawler(id_malware,date_added, url_malware, url_status, threat, tags, urlhaus_link, reporter_malware):
    
    """
    Add malware information into database.
    Params:
    - id_malware,date_added, url_malware, url_status, threat, tags, urlhaus_link, reporter_malware:
    Returns:
    -
    """
    client, db = connect_database()
    
    add_malware_collection(str(id_malware),
                           "urlhaus",
                           str(date_added),
                           str(url_malware),
                           url_status,
                           str(threat),
                           str(tags),
                           str(urlhaus_link),
                           str(reporter_malware)) 
    
    ip, country_name, country_code, continent_name, continent_code, region_name, region_code, city, zip, latitude, longitude, full_report = get_ip_geo_from_url(url_malware) 
    add_ip_collection(id_malware,
                      ip,
                      country_name,
                      country_code,
                      continent_name,
                      continent_code,
                      region_name,
                      region_code,
                      city,
                      zip,
                      latitude,
                      longitude,
                      str(full_report))

    """ CHECK IF ONLINE OR OFFLINE """
    if url_status == True:
                    
        """ SAVE FILE """
        id_file = file_from_url_to_gridfs(url_malware, id_malware)
        if id_file is not None:
            file_from_db_to_hd(db, id_malware)
            if is_exe(tags):
                db.malware_collection.update(
                { "id_malware" : id_malware },
                {"$set": { "imphash_malware" : get_imphash(id_malware)}})
            db.malware_collection.update( 
                { "id_malware" : id_malware },
                {"$set" :
                    {
                    "md5_malware": get_md5(id_malware),
                    "sha256_malware": get_sha256(id_malware),
                    "sha1_malware": get_sha1(id_malware), 
                    "ssdeep_malware": get_ssdeep(id_malware)
                    }
                }
            )

            os.remove("report/" + id_malware)
    
    client.close()

def parse_csv_urlhaus(db, request):
    """
    Parsing csv file from urlhaus
    Params:
    - url: (type: string).
    Returns:
    - name: (type: string).
    """
    print("Parsing csv file from urlhaus...")
    url_list = []
    lines = request.text.split('\n')

    reader = break_csv(lines)
    count = 0
    for item in reader:
        count = count + 1
        if count == 10:
            input_user = input("\nContinue? Y/N")
            if input_user == "Y":
                count = 0
            else:
                sys.exit()
        if len(item) > 1:
            id_malware, date_added, url_malware, url_status, threat, tags, urlhaus_link, reporter = save_infos_from_csv(item)            

            """ CHECK EXISTENCE IN DATABASE """
            if fp.id_malware_exist(db, id_malware) == False:
                
                add_malware_to_crawler.delay(id_malware,
                                       date_added,
                                       url_malware,
                                       url_status,
                                       threat,
                                       tags,
                                       urlhaus_link,
                                       reporter)
            else:
                print ("File ID " + id_malware + " already in DB")

def parse_csv_urlhaus2(db, request, stop):
    """
    Parsing csv file from urlhaus
    Params:
    - url: (type: string).
    Returns:
    - name: (type: string).
    """
    print("Parsing csv file from urlhaus...")
    url_list = []
    lines = request.text.split('\n')

    reader = break_csv(lines)
    count = 0
    for item in reader:
        if stop():
            if len(item) > 1:
                id_malware, date_added, url_malware, url_status, threat, tags, urlhaus_link, reporter = save_infos_from_csv(item)            

                """ CHECK EXISTENCE IN DATABASE """
                if fp.id_malware_exist(db, id_malware) == False:

                    add_malware_to_crawler.delay(id_malware,
                                           date_added,
                                           url_malware,
                                           url_status,
                                           threat,
                                           tags,
                                           urlhaus_link,
                                           reporter)
                else:
                    print ("File ID " + id_malware + " already in DB")

def init_connection_urlhaus():
    """ Initialisation, connection to Url Haus 
    Params:
    Returns:
    """
    try:
        logging.info('Fetching latest URLhaus list...')
        request = requests.get('https://urlhaus.abuse.ch/downloads/csv_recent/')
        if request.status_code == 200:
            logging.info('Processing URLhaus list...')
            return request
        else:
            logging.error('Problem connecting to URLhaus. Status code:{0}. Please try again later.'.format(request.status_code))

    except requests.exceptions.ConnectionError as e: logging.warning('Problem connecting to URLhaus. Error: {0}'.format(e))

    except Exception as e:
        logging.warning('Problem connecting to URLhaus. Aborting task.')
        logging.exception(sys.exc_info())
        logging.exception(type(e))
        logging.exception(e.args)
        logging.exception(e)

    return []


def clean_url(url):
    """Remove extraneous characters from URL.
    Params:
    - url: (type: string) URL to clean.
    Returns:
    - url: (type: string) clean URL.
    """

    if url is None:
        return None

    if '??' in url:
        url = url.split('??')[0]

    if url.endswith('?'):
        url = url[:-1]

    if '`' in url:
        url = url.replace('`', '')

    return url

